{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at finetuning Whisper. The Whisper model released in [September 2022](https://openai.com/blog/whisper/) by the authors Alec Radford et al. from OpenAI. Whisper is an encoder-decoder model pre-trained on 680k hours of labelled audio-transcription data. It achieves strong performance on many speech recognition and speech translation datasets without fine-tuning. \n",
    "\n",
    "In ðŸ¤— Transformers, the Whisper model has an associated feature extractor and tokenizer, \n",
    "called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
    "and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer) \n",
    "respectively. To make our lives simple, these two objects are wrapped under a single class, called the [WhisperProcessor](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor). We can call the WhisperProcessor to perform \n",
    "both the audio pre-processing and the text token post-processing. In doing so, we only need to keep track of two objects during training: \n",
    "the `processor` and the `model`.\n",
    "\n",
    "The Whisper checkpoints come in five configurations of varying model sizes. The smallest four are trained on either English-only or multilingual data.\n",
    "The largest checkpoint is multilingual only. All nine of the pre-trained checkpoints are available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The checkpoints are summarised in the following table with links to the models on the Hub:\n",
    "\n",
    "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
    "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
    "| tiny   | 4      | 384   | 6     | 39 M       | [âœ“](https://huggingface.co/openai/whisper-tiny.en)   | [âœ“](https://huggingface.co/openai/whisper-tiny.)  |\n",
    "| base   | 6      | 512   | 8     | 74 M       | [âœ“](https://huggingface.co/openai/whisper-base.en)   | [âœ“](https://huggingface.co/openai/whisper-base)   |\n",
    "| small  | 12     | 768   | 12    | 244 M      | [âœ“](https://huggingface.co/openai/whisper-small.en)  | [âœ“](https://huggingface.co/openai/whisper-small)  |\n",
    "| medium | 24     | 1024  | 16    | 769 M      | [âœ“](https://huggingface.co/openai/whisper-medium.en) | [âœ“](https://huggingface.co/openai/whisper-medium) |\n",
    "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large)  |\n",
    "\n",
    "When fine-tuning on an English dataset for speech recognition, it is recommeneded to select one of the English-only checkpoints. For any other language, it is recommended to select a multilingual checkpoint.\n",
    "\n",
    "\n",
    "The pipeline can be de-composed into three stages: \n",
    "1) A feature extractor which pre-processes the raw audio-inputs\n",
    "2) The model which performs the sequence-to-sequence mapping \n",
    "3) A tokenizer which post-processes the model outputs to text format\n",
    "\n",
    "\n",
    "We will be using the tiny multilingual checkpoint of the model, so let's set the parameter  `\"language\"` to our target text language. And since Whisper is a multimodal model we set the task to `\"transcribe\"` for speech recogntition. Another option would be `\"translate\"` for speech translation, but we will stick ASR for this part. These arguments modify the behaviour of the tokenizer - they should be set correctly to ensure the target labels are encoded properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Icelandic\", task=\"transcribe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset with streaming mode enabled. This will allow us to load the dataset in a memory-efficient way, and will allow us to use the `map` method to apply the pre-processing function to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('language-and-voice-lab/samromur_asr', streaming=True)\n",
    "train_ds= dataset[\"train\"]\n",
    "\n",
    "\n",
    "for idx, line in enumerate(train_ds):\n",
    "    print(line.keys())\n",
    "\n",
    "    if idx == 3:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our input audio is sampled at 16kHz, we need don't need to _downsample_ it to prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.  But that could be done by running the following code on the audio column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "train_ds = train_ds.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our pre-processing strategy. It's not advised that we do lower-case the transcriptions or remove punctuation unless mixing different datasets. This will enable you to fine-tune Whisper models that can predict punctuation and casing. Later, you will see how we can evaluate the predictions without punctuation or casing, so that the models benefit from the WER improvement obtained by normalising the transcriptions while still predicting fully formatted transcriptions. But since this version of Samromur is already lower cased we can't take advantage of this. In the offical publication of SamrÃ³mur it has the cased text as well as the noramlized text we use in this example.  \n",
    "\n",
    "\n",
    "A general preprocessing flow will look something like the following and it can all be written as a function:\n",
    "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, ðŸ¤— Datasets performs any necessary resampling operations on the fly.\n",
    "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
    "3. We perform any optional pre-processing (lower-case or remove punctuation).\n",
    "4. We encode the transcriptions to label ids through the use of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "do_lower_case = False\n",
    "do_remove_punctuation = False\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and (possibly) resample audio data to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    \n",
    "\n",
    "    if \"text\" in batch:\n",
    "        transcription = batch[\"text\"]\n",
    "    elif \"sentence\" in batch:\n",
    "        transcription = batch[\"sentence\"]\n",
    "    elif \"normalized_text\" in batch:\n",
    "        transcription = batch[\"normalized_text\"]\n",
    "    elif \"transcript\" in batch:\n",
    "        transcription = batch[\"transcript\"]\n",
    "    elif \"transcription\" in batch:\n",
    "        transcription = batch[\"transcription\"]\n",
    "\n",
    "    # optional pre-processing steps\n",
    "    if do_lower_case:\n",
    "        transcription = transcription.lower()\n",
    "    if do_remove_punctuation:\n",
    "        transcription = normalizer(transcription).strip()\n",
    "    \n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(transcription).input_ids\n",
    "    return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the data preparation function to all of our training examples using ðŸ¤— Datasets' `.map` method. We'll remove all of the columns from the raw training data, leaving just the `input_features` and `labels` defined in the `prepare_dataset` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = train_ds.map(prepare_dataset, remove_columns=[\"speaker_id\", \"audio\",\"gender\",\"audio_id\", \"age\", \"duration\"]).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, line in enumerate(vectorized_datasets):\n",
    "    print(line.keys())\n",
    "\n",
    "    if idx == 1:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define how we shuffle the data in the train split. The size of the subset we load is set by the variable `buffer_size`. You can increase or decrease this depending on your memory constraints. In this example, the `buffer_size` is set to 500, meaning 500 samples are loaded before shuffling across the subset. The larger we set this value, the closer to True offline shuffling. The `seed` is set for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.shuffle(\n",
    "    buffer_size=500,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we filter any training data with audio samples longer than 30s. These samples would otherwise be truncated by the Whisper feature-extractor which could affect the stability of training. We define a function that returns `True` for samples that are less than 30s, and `False` for those that are longer. We apply our filter function to all samples of our training dataset through ðŸ¤— Datasets' `.filter` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we've prepared our data, we're ready to dive into the training pipeline. \n",
    "The [ðŸ¤— Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)\n",
    "will do much of the heavy lifting for us. All we have to do is:\n",
    "\n",
    "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
    "\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We need to define a `compute_metrics` function that handles this computation.\n",
    "\n",
    "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "- Define the training configuration: this will be used by the ðŸ¤— Trainer to define the training schedule.\n",
    "\n",
    "### Define a Data Collator\n",
    "\n",
    "The data collator for a sequence-to-sequence speech model is unique in the  that it \n",
    "treats the `input_features` and `labels` independently: the  `input_features` must be \n",
    "handled by the feature extractor and the `labels` by the tokenizer.sense\n",
    "\n",
    "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram \n",
    "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
    "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
    "\n",
    "The `labels` on the other hand are un-padded. We first pad the sequences\n",
    "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens \n",
    "are then replaced by `-100` so that these tokens are **not** taken into account when \n",
    "computing the loss. We then cut the BOS token from the start of the label sequence as we \n",
    "append it later during training.\n",
    "\n",
    "We can leverage the `WhisperProcessor` we defined earlier to perform both the \n",
    "feature extractor and the tokenizer operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Load a Pre-Trained Checkpoint\n",
    "\n",
    "\n",
    "Override generation arguments - no tokens are forced as decoder outputs (see [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), no tokens are suppressed during generation (see [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)). Set `use_cache` to False since we're using gradient checkpointing, and the two are incompatible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluation metric and load evaluation data\n",
    "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We need to define a `compute_metrics` function that handles this computation.\n",
    "\n",
    "We then simply have to define a function that takes our model \n",
    "predictions and returns the WER metric. This function, called\n",
    "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
    "in the `label_ids` (undoing the step we applied in the \n",
    "data collator to ignore padded tokens correctly in the loss).\n",
    "It then decodes the predicted and label ids to strings. Finally,\n",
    "it computes the WER between the predictions and reference labels. \n",
    "Here, we have the option of evaluating with the 'normalised' transcriptions \n",
    "and predictions. We recommend you set this to `True` to benefit from the WER \n",
    "improvement obtained by normalising the transcriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "# If we have large test set it can be unpractical to evaluate the whole set at once. So let's d\n",
    "# define a maximum number of samples to evaluate on. \n",
    "max_eval_samples = 100\n",
    "\n",
    "#Â evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if do_normalize_eval:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "        # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "        pred_str = [pred_str[i] for i in range(len(pred_str)) if len(label_str[i]) > 0]\n",
    "        label_str = [label_str[i] for i in range(len(label_str)) if len(label_str[i]) > 0]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = datasets.load_dataset('google/fleurs', \"is_is\", split=\"test\", streaming=True)\n",
    "test_dataset = test_dataset.take(max_eval_samples)\n",
    "\n",
    "vectorized_test_dataset = test_dataset.map(prepare_dataset).with_format(\"torch\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training Configuration\n",
    "\n",
    "In the final step, we define all the parameters related to training. Here, you can set the `max_steps` to train for longer. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# Documenation for Seq2SeqTrainingArguments: https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainingarguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-tiny\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=250,\n",
    "    max_steps=1500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_steps=500,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a custom [Callback](https://huggingface.co/docs/transformers/main_classes/callback) that is called by the ðŸ¤— Trainer on the end of each epoch. The Callback reinitialises and reshuffles the streaming dataset at the beginning of each new epoch - this gives different shuffling across our subsets for every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=vectorized_datasets,\n",
    "    eval_dataset=vectorized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[ShuffleCallback()],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the model and processor to the output directory before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4385b5f70bf01ac7963dc78885efc7c7fb028bcc7ab6e1e0fbbdb01f861ac47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
