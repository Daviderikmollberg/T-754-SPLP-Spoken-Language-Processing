{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Datasets\n",
    "Hugging Face Datasets is a library for loading and preprocessing a wide range of NLP and computer vision datasets in a consistent and easy-to-use way. It provides a high-level API for loading, splitting, and processing various datasets, including popular benchmark datasets for natural language processing tasks such as text classification, question answering, and machine translation, as well as computer vision datasets for image classification, object detection, and semantic segmentation.\n",
    "\n",
    "The library is built on top of the PyTorch data loading utilities and is designed to work seamlessly with other PyTorch libraries and tools. The datasets are optimized for use with deep learning models and are preprocessed to provide a clean and consistent format for training, validation, and testing data.\n",
    "\n",
    "\n",
    "Start by creating an account on [Hugging Face](https://huggingface.co/) if you don't already have an account. Then, install the library with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install librosa\n",
    "!pip install matplotlib\n",
    "!pip install nlpaug"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing and Viewing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "ds_list = datasets.list_datasets()\n",
    "print(f\"There are {len(ds_list)} datasets in the library.\")\n",
    "\n",
    "asr_datasets = [ds for ds in ds_list if 'asr' in ds.lower()]\n",
    "print(f\"There are {len(asr_datasets)} with ASR in the name\\n\")\n",
    "print(\"\\n\".join(asr_datasets[:10]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one dataset. We'll use the SamrÃ³mur Speech Corpus, which is a collection of crowdsourced promted speeches. It contains about 100 thousand utterances for various speakers. The dataset is available on the Language and voice lab page on HuggingFace, and we can load it with the load_dataset function. The dataset is loaded as a DatasetDict, which is a dictionary-like object that contains multiple datasets. The actuatly files will be stored in the .chache directory in your home directory. \n",
    "\n",
    "Let's look at the features of the dataset. They can also be viewed on the [Hugging Face page](https://huggingface.co/datasets/language-and-voice-lab/samromur_asr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('language-and-voice-lab/samromur_asr', num_proc=5)\n",
    "\n",
    "print(dataset.keys())\n",
    "\n",
    "print(dataset['train'].description)\n",
    "for i in dataset[\"train\"].features.items():\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate over the features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=3\n",
    "print(\"\\t\".join(dataset[\"train\"][0].keys()))\n",
    "for idx,item in enumerate(dataset[\"train\"]):\n",
    "    print(f\"{item['audio_id']}\\t{item['speaker_id']}\\t{item['gender']}\\t{item['age']}\\t{item['duration']}\\t{item['normalized_text']}\")\n",
    "    if idx ==n:\n",
    "        break\n",
    "\n",
    "print(\"\\nThe audio key has a dict with the path to the file, numpy array of floats corresponding to the audio file and the sampling rate.\")\n",
    "for idx,item in enumerate(dataset[\"train\"]):\n",
    "    print(f\"{item['audio']}\")\n",
    "    if idx ==n:\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"\\nWe can verify that array is indeed the audio file by comparing the audio duration to the array length divided by the sampling rate.\")\n",
    "for idx,item in enumerate(dataset[\"train\"]):\n",
    "    print(f\"{item['duration']}\\t{len(item['audio']['array'])/item['audio']['sampling_rate']}\")\n",
    "    if idx ==n:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run standard preprocessing steps on the dataset. For example, we can lower case the text or remove puncuation. We can also do audio manipulation, such as resampling the audio to a different sample rate, feature extraction and augmentation. Let's run through a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa as lb\n",
    "import librosa.display \n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import nlpaug.augmenter.audio as naa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5\n",
    "sentence = dataset[\"train\"][n][\"normalized_text\"]\n",
    "audio_array = dataset[\"train\"][n][\"audio\"][\"array\"]\n",
    "sample_rate = dataset[\"train\"][n][\"audio\"][\"sampling_rate\"]\n",
    "audio_path = dataset[\"train\"][n][\"audio\"][\"path\"]\n",
    "print(\"Sentence:\", sentence)\n",
    "print(audio_array.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3)\n",
    "\n",
    "# Let's first plot the audio array\n",
    "lb.display.waveshow(audio_array, sr=sample_rate, ax=ax[0], x_axis='time')\n",
    "ax[0].set(title=sentence)\n",
    "ax[0].label_outer()\n",
    "\n",
    "# Then calculate and plot the mel spectrogram for the same audio file \n",
    "mel_spectrogram = lb.feature.melspectrogram(y=audio_array, sr=sample_rate, power=0.2)\n",
    "print(mel_spectrogram.shape)\n",
    "img = lb.display.specshow(mel_spectrogram, x_axis='time', y_axis='linear', ax=ax[1])\n",
    "ax[1].set(title=\"Mel spectogram\")\n",
    "ax[1].label_outer()\n",
    "\n",
    "# Finally, let's use nlpaug to augment the audio file and plot the result\n",
    "# More augmention methods can be found here:\n",
    "# https://github.com/makcedward/nlpaug/examples\n",
    "aug = naa.NoiseAug()\n",
    "augmented_array = np.array(aug.augment(audio_array))\n",
    "lb.display.waveshow(augmented_array, sr=sample_rate, ax=ax[2])\n",
    "ax[2].set(title=\"Augemented audio\")\n",
    "ax[2].label_outer()\n",
    "\n",
    "\n",
    "# Let's add a player as well\n",
    "ipd.Audio(audio_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go over an issue with our current setup.\n",
    "When we training good ASR models, we often need hundreds or thosands of hours of data. A typical speech dataset consists of approximately 100 hours of audio-transcription data, requiring upwards of 130GB of storage space for download and preparation. For most ASR researchers, this is already at the upper limit of what is feasible for disk space. So what happens when we want to train on a larger dataset? The full [LibriSpeech](https://huggingface.co/datasets/librispeech_asr) dataset consists of 960 hours of audio data. Do we need to bite the bullet and buy additional storage? Or is there a way we can train on all of these datasets with no disk drive requirements?\n",
    "\n",
    "When training machine learning systems, we rarely use the entire dataset at once. We typically _batch_ our data into smaller subsets of data, and pass these incrementally through our training pipeline. This is because we train our system on an accelerator device, such as a GPU or TPU, which has a memory limit typically around 12GB. We have to fit our model, optimiser and training data all on the same accelerator device, so we usually have to divide the dataset up into smaller batches and move them from the CPU to the GPU when required.\n",
    "\n",
    "Consequently, we don't require the entire dataset to be downloaded at once; we simply need the batch of data that we pass to our model at any one go. We can leverage this principle of partial dataset loading when preparing our dataset: rather than downloading the entire dataset at the start, we can load each piece of data as and when we need it. For each batch, we load the relevant data from a remote server and pass it through the training pipeline. For the next batch, we load the next items and again pass them through the training pipeline. At no point do we have to save data to our disk drive, we simply load them in memory and use them in our pipeline. In doing so, we only ever need as much memory as each individual batch requires.\n",
    "\n",
    "This is analogous to downloading a TV show versus streaming it ðŸ“º When we download a TV show, we download the entire video offline and save it to our disk. Compare this to when we stream a TV show. Here, we don't download any part of the video to memory, but iterate over the video file and load each part in real-time as required. It's this same principle that we can apply to our ML training pipeline! We want to iterate over the dataset and load each sample of data as required.\n",
    "\n",
    "The Hugging Face setup alows us to do this easily (given that the acutal data repository is setup in a compatible manner). \n",
    "\n",
    "\n",
    "Let's first check out the memory footprint of SamrÃ³mur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh ~/.cache/huggingface/datasets/*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do a minor change to the code above. We will now load the dataset in a streaming fashion, this is done by setting the streaming parameter to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('language-and-voice-lab/samromur_asr', streaming=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to clear SamrÃ³mur out of the cache directory, you can do so by running the following command, but this will also delete all other Hugging Face datasets you have downloaded. \n",
    "\n",
    "!rm -r ~/.cache/huggingface/datasets/downloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
